#  PDF RAG QA

Retrieval-Augmented Generation backend for question-answering over your own PDFs using **FastAPI**, **Sentence Transformers (allâ€‘MiniLMâ€‘L6â€‘v2)**, and **ChromaDB**. No OpenAI key required. 

---

##  Stack

- FastAPI + Uvicorn [web:114]  
- SentenceTransformers `all-MiniLM-L6-v2` (384â€‘d embeddings) 
- ChromaDB vector store (persistent)   
- pypdf for PDF text extraction  
- Python 3.12

---

##  Structure

rag-pdf-qa/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api/ # ingest, query endpoints
â”‚ â”œâ”€â”€ core/ # config, logging
â”‚ â”œâ”€â”€ models/ # Pydantic schemas
â”‚ â”œâ”€â”€ services/ # pdf â†’ text â†’ chunks â†’ embeddings â†’ Chroma
â”‚ â”œâ”€â”€ rag/ # RAG pipeline + prompt
â”‚ â””â”€â”€ main.py # FastAPI app
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ uploads/ # raw PDFs (gitignored)
â”‚ â””â”€â”€ chroma/ # Chroma index (gitignored)
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

##  RAG Flow

1. **Ingest (`POST /ingest`)**  
   - Upload PDF â†’ extract text (pypdf).  
   - Normalize text, split into 800â€‘char chunks with 200 overlap.  
   - Encode with `all-MiniLM-L6-v2` â†’ 384â€‘d embeddings. 
   - Store chunks + embeddings in Chroma.

2. **Query (`POST /query`)**  
   - Embed question with same model.  
   - Chroma topâ€‘k similarity search returns relevant chunks. 
   - Current version returns an extractive answer by concatenating those chunks (LLMâ€‘agnostic).
  
## ðŸ›  Ideas

- Plug an LLM into `app/rag/pipeline.py` for generated answers.  
- Add a small frontend (React/Next/Streamlit) on top of this API.
